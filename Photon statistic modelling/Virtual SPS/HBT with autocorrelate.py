##############################################################################
# Import some libraries
##############################################################################
import sys
import os
import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import curve_fit

##############################################################################
# Import some extra special libraries from my own repo and do some other stuff
##############################################################################
sys.path.insert(0, r"C:\local files\Python\Local Repo\library")
import prd_data_proc
import prd_plots
import prd_maths
np.set_printoptions(suppress=True)
cs = prd_plots.palette()

##############################################################################
# Import generated data
##############################################################################
# load photon timing files generated by 'Generate photons.py'
T = 15
γs = 1e6
p0 = r'C:\local files\Experimental Data\G5 A5 Python simulations'\
    r'\Single photon statistics\Data\20191202'

f0 = p0 + r'\1e' + str(int(np.log10(γs))) + ' Photons, ' + \
    'T = ' + str(int(T * 100)) + ' taus abs.txt'
f1 = p0 + r'\1e' + str(int(np.log10(γs))) + ' Photons, ' + \
    'T = ' + str(int(T * 100)) + ' taus rel.txt'
f2 = p0 + r'\1e' + str(int(np.log10(γs))) + ' Photons, ' + \
    'T = ' + str(int(T * 100)) + ' taus 1.txt'
f3 = p0 + r'\1e' + str(int(np.log10(γs))) + ' Photons, ' + \
    'T = ' + str(int(T * 100)) + ' taus 2.txt'
f4 = p0 + r'\1e' + str(int(np.log10(γs))) + ' Photons, ' + \
    'T = ' + str(int(T * 100)) + ' taus HBT.txt'
f5 = p0 + r'\1e' + str(int(np.log10(γs))) + ' Photons, ' + \
    'T = ' + str(int(T * 100)) + ' fom.txt'

τs_abs = np.loadtxt(f0)
τs_rel = np.loadtxt(f1)
τs_1 = np.loadtxt(f2)
τs_2 = np.loadtxt(f3)
τs_HBT = np.loadtxt(f4)
fom = np.loadtxt(f5)
τ = fom[0]
Δt = fom[1]
γs = fom[2]
t_max = np.max(τs_abs)
print(np.shape(τs_abs))
γ_rate = γs / t_max
print('total time = ', t_max / 1e9, 's')
print('total photons = ', γs)
print('total count rate = ', γ_rate * 1e6, ' kcps')

# trim longer array to length of shorter
n = min(len(τs_1), len(τs_2))
τs_1 = τs_1[0:n]
τs_2 = τs_2[0:n]

#######################################################################
# Do alternative 'beam splitting' by randomly allocating
# τs_abs elements to alt_t1 & alt_t2
#######################################################################

alt_t1 = []
alt_t2 = []
for i0, v0 in enumerate(τs_abs[0:5000]):
    if np.random.random() < 0.5:
        alt_t1.append(v0)
    else:
        alt_t2.append(v0)

#######################################################################
# Generate time series based on photon arrival times
#######################################################################

ts1 = np.zeros(int(np.max(alt_t1)))
ts2 = np.zeros(int(np.max(alt_t2)))
# print(np.shape(ts1))
# print(np.shape(ts2))
for i0, v0 in enumerate(alt_t1):
    ts1[int(v0) - 1] = 1

for i0, v0 in enumerate(alt_t2):
    ts2[int(v0) - 1] = 1

# print(ts1[0:100])

# alt_τ = np.linspace(-1000, 1000, 2001)
# t1_bar = len(alt_t1) / np.max(alt_t1)
# t2_bar = len(alt_t2) / np.max(alt_t2)
# g2 = []
# for i0, v0 in alt_τ:
#     g2.append()
# g = np.correlate(ts1, ts2, 'full')


#######################################################################
# Analytic approach to correlation measurements
#######################################################################
# see prd_maths.g2_3_lvl functions
a = 0.1
b = 0.002
c = 1
d = 0.0005
τs = np.linspace(-1500, 1500, 5000)
init = [a, b, c, d]
# g2 function taken from "Berthel et al 2015"
fit = 1 - c * np.exp(- a * np.abs(τs)) \
    + (c - 1) * np.exp(- b * np.abs(τs))
exp_decay = (np.exp(- d * np.abs(τs)))
fit_total = fit * exp_decay

#######################################################################
# Plot some figures
#######################################################################
prd_plots.ggplot()

# histograms
x1 = τs_HBT
x1 = [i for i in x1 if i <= 1200]
x1 = [i for i in x1 if i >= -1200]
print(np.shape(x1)[0])

bin_N = 501
hist, bins = np.histogram(x1, bins=bin_N, density=False)
bin_centres = np.linspace(bins[0] + (bins[1] - bins[0]) / 2,
                          bins[-2] + (bins[-1] - bins[-2]) / 2, len(bins) - 1)
hist_time = np.linspace(bins[0] + (bins[1] - bins[0]) / 2,
                        bins[-2] + (bins[-1] - bins[-2]) / 2, 1000)

bin_width = bins[1] - bins[0]

# This normalisation is dicey. Normally you'd do:
# (4 * hist * t_max) / (γs * γs * bin_width) for an HBT.
# I'm doubling my HBT collection rate in the code so hist values
# are twice a normal stop start. This drops it by a factor of 2.
g2s_exp = 2 * (hist * t_max) / (γs * γs * bin_width)

# funciton histogram seems to divide by 100 somewhere
y1 = g2s_exp * 100

popt, pcov = curve_fit(prd_maths.g2_3_lvl_exp,
                       bin_centres, y1, p0=[*init])

# fig1 = plt.figure('fig1', figsize=(3 * np.sqrt(2), 3))
# ax1 = fig1.add_subplot(1, 1, 1)
# fig1.patch.set_facecolor(cs['mnk_dgrey'])
# ax1.set_xlabel('')
# ax1.set_ylabel('')
# plt.plot(g, '.')
# plt.tight_layout()
# plt.show()

fig2 = plt.figure('fig2', figsize=(3 * np.sqrt(2), 3))
ax2 = fig2.add_subplot(1, 1, 1)
fig2.patch.set_facecolor(cs['mnk_dgrey'])
ax2.set_xlabel('Time')
ax2.set_ylabel('#')
# ax2.set_yscale('log')
ax2.plot(bin_centres, y1, '.', color=cs['ggblue'])
# plt.hist(x2, bins=bin_N, alpha=0.5,
#          facecolor=cs['ggred'], edgecolor=cs['mnk_dgrey'],
#          label='Probabilistic')
# ax2.plot(τs, fit, '.', label='Analytic')
# ax2.plot(τs, exp_decay, '.', label='Analytic')
ax2.plot(τs, prd_maths.g2_3_lvl_exp(
    τs, *popt), '-',
    color=cs['ggred'],
    label='Fit',
    lw=0.5)
ax2.plot(τs, prd_maths.g2_3_lvl_exp(
    τs, *init), '-',
    color=cs['ggyellow'],
    label='Initial fit',
    lw=0.5)
# ax2.plot(bin_centres, x2, '.', color=cs['ggblue'])
ax2.legend(loc='upper right', fancybox=True, framealpha=0.5)
os.chdir(p0)
plt.savefig('plot.png')
plt.title('τs_HBT')
plt.tight_layout()
# plt.xlim(0, 1000)
plt.show()
